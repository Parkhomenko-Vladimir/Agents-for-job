{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54b5a667-c02d-481b-873f-3a9dc3b1fdb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions.categorical import Categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156eb1b5-90b0-42c9-b3e7-1c1c3961205b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2236a66-8577-4ea8-b1dd-605b369ef0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOMemory:\n",
    "    def __init__(self,batch_size):\n",
    "        self.states = []\n",
    "        self.probs = []\n",
    "        self.vals = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.dones = []\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def generate_butches(self):\n",
    "\n",
    "        return np.array(self.states), \\\n",
    "               np.array(self.actions), \\\n",
    "               np.array(self.probs), \\\n",
    "               np.array(self.vals), \\\n",
    "               np.array(self.rewards), \\\n",
    "               np.array(self.dones)\n",
    "               # batches\n",
    "\n",
    "    def store_memory(self, state, action, probs, vals, reward, done):\n",
    "        self.states.append(state)\n",
    "        self.probs.append(probs)\n",
    "        self.vals.append(vals)\n",
    "        self.rewards.append(reward)\n",
    "        self.actions.append(action)\n",
    "        self.dones.append(done)\n",
    "\n",
    "    def clear_memory(self):\n",
    "        self.states = []\n",
    "        self.probs = []\n",
    "        self.vals = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.dones = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5022620-f570-47de-9d59-0a70f7f5f765",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Actor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48562eb3-8085-4c59-9626-728cc005831f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorNetwork(nn.Module):\n",
    "    def __init__(self,n_actions, input_dims, alpha,\n",
    "                 chkpt_dir='tmp/ppo'):\n",
    "        super(ActorNetwork, self).__init__()\n",
    "\n",
    "        self.checkpoint_file = os.path.join(chkpt_dir, 'actor_ppo')\n",
    "        \n",
    "        self.l1 = nn.Linear(*input_dims, 256)\n",
    "        self.r1 = nn.ReLU()\n",
    "        self.l2 = nn.Linear(256, 256)\n",
    "        self.r2 = nn.ReLU()\n",
    "        self.l3 = nn.Linear(256,n_actions)\n",
    "        self.st = nn.Softmax(dim=-1)   \n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=alpha)\n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self,state):\n",
    "        x = self.r1(self.l1(state))\n",
    "        x = self.r2(self.l2(x))\n",
    "        x = self.st(self.l3(x))\n",
    "        dist = Categorical(x)\n",
    "\n",
    "        return dist\n",
    "\n",
    "    def save_checkpoint(self):\n",
    "        T.save(self.state_dict(), self.checkpoint_file)\n",
    "\n",
    "    def load_checkpoint(self):\n",
    "        self.load_state_dict(T.load(self.checkpoint_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559dd009-bbfb-42ab-9b4c-422f2953d9a2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "171f5a72-7afd-4e61-8016-bd37bb5141cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CriticNetwork(nn.Module):\n",
    "    def __init__(self, input_dims, alpha,  chkpt_dir='tmp/ppo'):\n",
    "        super(CriticNetwork, self).__init__()\n",
    "\n",
    "        self.checkpoint_file = os.path.join(chkpt_dir, 'critic_ppo')\n",
    "        self.critic = nn.Sequential(\n",
    "                nn.Linear(*input_dims, 256),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(256, 256),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(256,1),\n",
    "                nn.Softmax(dim=-1)\n",
    "        )\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=alpha)\n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self,state):\n",
    "        value = self.critic(state)\n",
    "\n",
    "        return value\n",
    "\n",
    "    def save_checkpoint(self):\n",
    "        T.save(self.state_dict(), self.checkpoint_file)\n",
    "\n",
    "    def load_checkpoint(self):\n",
    "        self.load_state_dict(T.load(self.checkpoint_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b1bdb3-4918-45f7-98f7-a619dfe4fe0d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "280b8ead-054d-400b-84e9-38c83e76ace4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, n_actions, input_dims, gamma=0.99, alpha=0.003, gae_lambda=0.95,\n",
    "                 policy_clip=0.2, batch_size=64, N=2048, n_epochs=10):\n",
    "        self.gamma = gamma\n",
    "        self.policy_clip = policy_clip\n",
    "        self.n_epochs = n_epochs\n",
    "        self.gae_lambda = gae_lambda\n",
    "\n",
    "        self.actor = ActorNetwork(n_actions = n_actions, input_dims = input_dims, alpha = alpha)\n",
    "        self.critic = CriticNetwork(input_dims, alpha = alpha)\n",
    "        self.memory = PPOMemory(batch_size)\n",
    "\n",
    "    def remember(self, state, action, probs, vals, reward, done):\n",
    "        self.memory.store_memory(state, action, probs, vals, reward, done)\n",
    "\n",
    "    def save_models(self):\n",
    "        print('... saving models ...')\n",
    "        self.actor.save_checkpoint()\n",
    "        self.critic.save_checkpoint()\n",
    "\n",
    "    def load_models(self):\n",
    "        print('... loading models ...')\n",
    "        self.actor.load_checkpoint()\n",
    "        self.critic.load_checkpoint()\n",
    "\n",
    "    def choose_action(self, observation):\n",
    "        state = T.tensor([observation], dtype=T.float).to(self.actor.device)\n",
    "\n",
    "        dist = self.actor(state)\n",
    "        value = self.critic(state)\n",
    "        action = dist.sample()\n",
    "\n",
    "        probs = T.squeeze(dist.log_prob(action)).item()\n",
    "        action = T.squeeze(action).item()\n",
    "        value = T.squeeze(value).item()\n",
    "\n",
    "        return action, probs, value\n",
    "\n",
    "    def learn(self):\n",
    "        for i in range(10):\n",
    "            state_arr, action_arr, old_probs_arr, vals_arr, \\\n",
    "            reward_arr, done_arrs = self.memory.generate_butches()\n",
    "\n",
    "            values = vals_arr\n",
    "            advantage = np.zeros(len(reward_arr), dtype = np.float32)\n",
    "\n",
    "            for t in range(len(reward_arr)-1):\n",
    "                discount = 1\n",
    "                a_t = 0\n",
    "                for k in range(t, len(reward_arr)-1):\n",
    "                    a_t += discount*(reward_arr[k] + self.gamma*values[k+1]* \\\n",
    "                                    (1-int(done_arrs[k])) - values[k])\n",
    "                    discount *= self.gamma*self.gae_lambda\n",
    "                advantage[t] = a_t\n",
    "            advantage = T.tensor(advantage).to(self.actor.device)\n",
    "\n",
    "            values = T.tensor(values).to(self.actor.device)\n",
    "            states = T.tensor(state_arr, dtype=T.float).to(self.actor.device)\n",
    "            old_probs = T.tensor(old_probs_arr).to(self.actor.device)\n",
    "            actions = T.tensor(action_arr).to(self.actor.device)\n",
    "\n",
    "            dist = self.actor(states)\n",
    "\n",
    "            critic_value = self.critic(states)\n",
    "\n",
    "            critic_value = T.squeeze(critic_value)\n",
    "\n",
    "            new_probs = dist.log_prob(actions)\n",
    "            # prob_ratio = new_probs.exp() / old_probs.exp()\n",
    "            prob_ratio = (new_probs - old_probs).exp()\n",
    "            weight_probs = advantage * prob_ratio\n",
    "            weight_clipped_probs = T.clamp(prob_ratio, 1-self.policy_clip,\n",
    "                    1+self.policy_clip)*advantage\n",
    "            actor_loss = -T.min(weight_probs, weight_clipped_probs).mean()\n",
    "\n",
    "            returns = advantage + values\n",
    "            critic_loss = (returns-critic_value)**2\n",
    "            critic_loss = critic_loss.mean()\n",
    "\n",
    "            total_loss = actor_loss + 0.5*critic_loss\n",
    "            self.actor.optimizer.zero_grad()\n",
    "            self.critic.optimizer.zero_grad()\n",
    "            total_loss.backward()\n",
    "            self.actor.optimizer.step()\n",
    "            self.critic.optimizer.step()\n",
    "\n",
    "        self.memory.clear_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03ba015-bea3-401f-9a59-d92152f01c0d",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7f3e253-d41e-40f5-82c8-f90e789bf02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy\n",
    "import numpy as np\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d1c00b8-baba-45d6-9e97-9132c15990fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode  1990 score 9.0 avg score 9.4 time step 130367 learning_steps 6518\n",
      "episode  1991 score 9.0 avg score 9.4 time step 130376 learning_steps 6518\n",
      "episode  1992 score 9.0 avg score 9.4 time step 130385 learning_steps 6519\n",
      "episode  1993 score 10.0 avg score 9.4 time step 130395 learning_steps 6519\n",
      "episode  1994 score 9.0 avg score 9.4 time step 130404 learning_steps 6520\n",
      "episode  1995 score 8.0 avg score 9.4 time step 130412 learning_steps 6520\n",
      "episode  1996 score 9.0 avg score 9.4 time step 130421 learning_steps 6521\n",
      "episode  1997 score 9.0 avg score 9.4 time step 130430 learning_steps 6521\n",
      "episode  1998 score 10.0 avg score 9.4 time step 130440 learning_steps 6522\n",
      "episode  1999 score 10.0 avg score 9.4 time step 130450 learning_steps 6522\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "N = 20\n",
    "batch_size = 5\n",
    "n_epochs = 4\n",
    "alpha = 0.003\n",
    "agent = Agent(n_actions=env.action_space.n, batch_size=batch_size,\n",
    "              alpha=alpha, n_epochs=n_epochs,\n",
    "              input_dims=env.observation_space.shape)\n",
    "\n",
    "n_games = 2000\n",
    "\n",
    "best_score = env.reward_range[0]\n",
    "score_history = []\n",
    "\n",
    "learn_iters = 0\n",
    "avg_score = 0\n",
    "n_steps = 0\n",
    "\n",
    "for i in range(n_games):\n",
    "    observation = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "    while not done:\n",
    "        action, prob, val = agent.choose_action(observation)\n",
    "        observation_, reward, done, info = env.step(action)\n",
    "        n_steps += 1\n",
    "        score += reward\n",
    "        agent.remember(observation, action, prob, val, reward, done)\n",
    "        if n_steps % N == 0:\n",
    "            agent.learn()\n",
    "            learn_iters += 1\n",
    "        observation = observation_\n",
    "    score_history.append(score)\n",
    "    avg_score = np.mean(score_history[-100:])\n",
    "\n",
    "    if avg_score > best_score:\n",
    "        best_score = avg_score\n",
    "        agent.save_models()\n",
    "    \n",
    "    if not bool(i%10):clear_output()\n",
    "    \n",
    "    print('episode ', i, 'score %.1f' % score, 'avg score %.1f' % avg_score,\n",
    "          'time step', n_steps, 'learning_steps', learn_iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658631d0-f92f-4d86-90c9-92218651ac04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9298fb-3e4b-44f6-811c-41fd3430ab66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f110fea0-5826-47fa-a447-9208b92f218a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88932bc-633c-4746-937a-4af01e88f32d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43be8e4d-36b1-452c-b9ab-2c293ab2e21f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b0e5a0-ea9f-4902-8fcc-43d6d0ba5499",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6606f80e-89a8-4fc8-bd62-59881de57829",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02ce0f0-cca6-4415-9fa6-3743e11036b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882f0380-1479-4716-8964-8cd8ba10178c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854d8a36-1b49-494c-b98a-4f26014b32fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a92bd19-d5e3-4075-ae38-bfd5486a783a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2dac82-1f0c-4c1e-a023-1af3e2fb8588",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee64f1a9-04f5-4753-a80f-b432f3db38eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6adccc-0cf7-4083-8c38-5e8bcdca7ae6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513ca4ec-8322-482e-bb00-6b8b325dde3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c31311b-41b7-4ec0-9e85-d8bca28dec2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
