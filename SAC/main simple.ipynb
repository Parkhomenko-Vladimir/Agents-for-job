{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a8bd08e-8611-4b68-af60-efed219303f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pybullet_envs\n",
    "import gym\n",
    "import numpy\n",
    "from sac import Agent\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb230901-b3ed-41e5-b803-170140b58f83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43048373-f3cf-4697-bcba-375b4ff066b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7cf35ed0-e6b3-4ff4-a8d5-3f81c4ad5da0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pybullet build time: Aug 19 2022 11:18:58\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for ValueNetwork:\n\tMissing key(s) in state_dict: \"value.4.b2.running_mean\", \"value.4.b2.running_var\", \"value.4.b4.running_mean\", \"value.4.b4.running_var\", \"value.6.b2.running_mean\", \"value.6.b2.running_var\", \"value.6.b4.running_mean\", \"value.6.b4.running_var\", \"value.8.b2.running_mean\", \"value.8.b2.running_var\", \"value.8.b4.running_mean\", \"value.8.b4.running_var\", \"value.10.b2.running_mean\", \"value.10.b2.running_var\", \"value.10.b4.running_mean\", \"value.10.b4.running_var\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m env_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInvertedPendulumBulletEnv-v0\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      2\u001b[0m env \u001b[38;5;241m=\u001b[39m gym\u001b[38;5;241m.\u001b[39mmake(env_id)\n\u001b[0;32m----> 3\u001b[0m agent \u001b[38;5;241m=\u001b[39m \u001b[43mAgent\u001b[49m\u001b[43m(\u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.003\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.003\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreward_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menv_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m              \u001b[49m\u001b[43minput_dims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobservation_space\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtau\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.005\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m              \u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer1_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer2_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m              \u001b[49m\u001b[43mn_actions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction_space\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m n_games \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m250\u001b[39m\n\u001b[1;32m      8\u001b[0m filename \u001b[38;5;241m=\u001b[39m env_id \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(n_games)\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgames_scale\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(agent\u001b[38;5;241m.\u001b[39mscale)\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[0;32m~/PycharmProjects/Rl/Ready/SAC/sac.py:37\u001b[0m, in \u001b[0;36mAgent.__init__\u001b[0;34m(self, alpha, beta, input_dims, tau, env, env_id, gamma, n_actions, max_size, layer1_size, layer2_size, batch_size, reward_scale)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_value \u001b[38;5;241m=\u001b[39m ValueNetwork(beta, input_dims, layer1_size,\n\u001b[1;32m     34\u001b[0m                                  layer2_size, name\u001b[38;5;241m=\u001b[39menv_id \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_target_value\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale \u001b[38;5;241m=\u001b[39m reward_scale\n\u001b[0;32m---> 37\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_network_parameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtau\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/PycharmProjects/Rl/Ready/SAC/sac.py:122\u001b[0m, in \u001b[0;36mAgent.update_network_parameters\u001b[0;34m(self, tau)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m value_state_dict:\n\u001b[1;32m    119\u001b[0m     value_state_dict[name] \u001b[38;5;241m=\u001b[39m tau \u001b[38;5;241m*\u001b[39m value_state_dict[name]\u001b[38;5;241m.\u001b[39mclone() \u001b[38;5;241m+\u001b[39m \\\n\u001b[1;32m    120\u001b[0m                              (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m tau) \u001b[38;5;241m*\u001b[39m target_value_state_dict[name]\u001b[38;5;241m.\u001b[39mclone()\n\u001b[0;32m--> 122\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_value\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue_state_dict\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1604\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1599\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   1600\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1601\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(k) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   1603\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 1604\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1605\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   1606\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for ValueNetwork:\n\tMissing key(s) in state_dict: \"value.4.b2.running_mean\", \"value.4.b2.running_var\", \"value.4.b4.running_mean\", \"value.4.b4.running_var\", \"value.6.b2.running_mean\", \"value.6.b2.running_var\", \"value.6.b4.running_mean\", \"value.6.b4.running_var\", \"value.8.b2.running_mean\", \"value.8.b2.running_var\", \"value.8.b4.running_mean\", \"value.8.b4.running_var\", \"value.10.b2.running_mean\", \"value.10.b2.running_var\", \"value.10.b4.running_mean\", \"value.10.b4.running_var\". "
     ]
    }
   ],
   "source": [
    "env_id = 'InvertedPendulumBulletEnv-v0'\n",
    "env = gym.make(env_id)\n",
    "agent = Agent(alpha=0.003, beta=0.003, reward_scale=2, env_id=env_id,\n",
    "              input_dims=env.observation_space.shape, tau=0.005,\n",
    "              env=env, batch_size=256, layer1_size=256, layer2_size=256,\n",
    "              n_actions=env.action_space.shape[0])\n",
    "n_games = 250\n",
    "filename = env_id + '_' +str(n_games)+'games_scale'+str(agent.scale)+'.png'\n",
    "figure_file = 'plots/' + filename\n",
    "\n",
    "best_score = env.reward_range[0]\n",
    "score_history = []\n",
    "load_checkpoint = False\n",
    "\n",
    "if load_checkpoint:\n",
    "    agent.load_madels()\n",
    "    env.render(mode='human')\n",
    "\n",
    "steps=0\n",
    "for i in range(n_games):\n",
    "    score = 0\n",
    "    done = False\n",
    "    observation = env.reset()\n",
    "\n",
    "    while not done:\n",
    "        action = agent.choose_actions(observation)\n",
    "        observation_, reward, done, info = env.step(action)\n",
    "        steps += 1\n",
    "        score += reward\n",
    "        agent.remember(observation,action, reward, observation_, done)\n",
    "        if not load_checkpoint:\n",
    "            agent.learn()\n",
    "        observation = observation_\n",
    "    score_history.append(score)\n",
    "    avg_score = np.mean(score_history[-100:])\n",
    "\n",
    "    if avg_score > best_score:\n",
    "        best_score = avg_score\n",
    "        if not load_checkpoint:\n",
    "            agent.save_models()\n",
    "            \n",
    "    if not bool(i%10):clear_output()\n",
    "    print('episode', i, 'score %.1f' % avg_score,\n",
    "          'training 100 games avg %.1f' % avg_score,\n",
    "          'steps %d' % steps, env_id, 'scale', agent.scale)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fb982f-d849-43d1-8696-98dffa24a1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curve(scores, x, figure_file):\n",
    "    running_avg = np.zeros(len(scores))\n",
    "    for i in range(len(running_avg)):\n",
    "        running_avg[i] = np.mean(scores[max(0, i-100):(i+1)])\n",
    "    plt.plot(x, running_avg)\n",
    "    plt.title('Average of previos 100 scores')\n",
    "    plt.savefig(figure_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16cf0066-1a79-4b37-9052-ef16ab3f64bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [i+1 for i in range(n_games)]\n",
    "    plot_learning_curve(x, score_history, figure_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fdfee39-e03f-40be-8c20-10962818509c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2581efb-3354-440e-96ba-0aab10d78df3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a539aa9d-0384-4822-ba57-932879b59f0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ecfbb3-d74b-4da5-9adb-2a402cdb001a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e995be11-13f8-4ec1-80dd-2d7ca843f9c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b126063b-ce44-4ea4-9323-2a9a1738e70f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18236d6-2150-4a46-b658-a27cef241217",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5bc86d-dbcc-43d6-a36c-2127ffea615a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
