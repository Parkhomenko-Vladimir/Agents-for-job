{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a8bd08e-8611-4b68-af60-efed219303f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "\n",
    "import numpy\n",
    "from sac import Agent\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7049ee4-caff-408c-a2be-248da0cadec6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Enviroment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43048373-f3cf-4697-bcba-375b4ff066b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.1.0 (SDL 2.0.16, Python 3.10.4)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "from Enviroment import Enviroment\n",
    "\n",
    "class CustomEnv(gym.Env):\n",
    "    '''\n",
    "    Оборочивание класса среды в среду gym\n",
    "    '''\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self, obstacle_turn: bool, Total_war: bool, num_obs: int, num_enemy: int, inp_dim: int,\n",
    "                 size_obs, steps_limit, vizualaze = False, head_velocity = 0.01, rew_col = -70,\n",
    "                 rew_win = 100, rew_defeat = -100):\n",
    "        '''\n",
    "        Инициализация класса среды\n",
    "        :param obstacle_turn: (bool) Флаг генерации препятствий\n",
    "        :param vizualaze: (bool) Флаг генерации препятствий\n",
    "        :param Total_war: (bool) Флаг режима игры (с противником или без)\n",
    "        :param steps_limit: (int) Максимальное количество действий в среде за одну игру\n",
    "        '''\n",
    "\n",
    "        self.log_koef = 50\n",
    "        self.ang_Norm_coef = np.pi\n",
    "        self.coords_Norm_coef = 500\n",
    "        \n",
    "        self.inp_dim = inp_dim\n",
    "        \n",
    "        self.rew_col = rew_col\n",
    "        self.rew_win = rew_win\n",
    "        self.rew_defeat = rew_defeat\n",
    "\n",
    "        self.enviroment = Enviroment(obstacle_turn, vizualaze, Total_war,\n",
    "                             head_velocity, num_obs, num_enemy, size_obs, steps_limit,\n",
    "                             rew_col, rew_win, rew_defeat,epsilon = 100,sigma =30)\n",
    "\n",
    "        self.enviroment.reset()\n",
    "\n",
    "        self.action_space = spaces.Discrete(8)\n",
    "        self.observation_space = gym.spaces.Box(low=0, high=255, shape=(self.inp_dim, self.inp_dim, 3), dtype=np.uint8)\n",
    "       \n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Метод осуществления шага в среде\n",
    "        :param action: (int) направление движения в среде\n",
    "        :return: dict_state, reward, not done, {}: состояние, реворд, флаг терминального состояния, информация о среде\n",
    "        \"\"\"\n",
    "        state, reward, done, numstep = self.enviroment.step(int(action))\n",
    "\n",
    "        x2 = state.posRobot[0]\n",
    "        y2 = state.posRobot[1]\n",
    "    \n",
    "        x4 = state.target[0,0]\n",
    "        y4 = state.target[0,1]\n",
    "        \n",
    "        \n",
    "        f2 =  state.target[0,2]\n",
    "        f2 = np.deg2rad(f2)\n",
    "        \n",
    "        Ax4, Ay4 = -np.cos(f2), np.sin(f2)\n",
    "        Bx24, By24 = x2 - x4, y2 - y4\n",
    "\n",
    "        dist = - np.sqrt(np.abs((x2-x4)**2 + (y2-y4)**2))\n",
    "        phy = (Ax4*Bx24 + Ay4*By24)/(np.sqrt(Ax4**2 + Ay4**2) * np.sqrt(Bx24**2 + By24**2))\n",
    "        reward_l = phy*(dist+500) * 0.01 * (not done) + np.round(reward, 2).sum()\n",
    "\n",
    "        return state.img, reward_l, done, {}\n",
    "        \n",
    "    def reset(self):\n",
    "        '''\n",
    "        Метод обновления игры\n",
    "        :return: dict_state: состояние\n",
    "        '''\n",
    "        state = self.enviroment.reset()\n",
    "\n",
    "        return state.img\n",
    "\n",
    "    def render(self, model, num_gifs=1):\n",
    "        '''\n",
    "        Метод вывода информации об игре\n",
    "        :param mode:\n",
    "        :return:\n",
    "        '''\n",
    "        for i in range(num_gifs):\n",
    "            \n",
    "            images = []\n",
    "            obs = self.reset()\n",
    "            img = obs['img']# env.render(mode='rgb_array')\n",
    "            done = False\n",
    "                \n",
    "            height, width, layers = img.shape\n",
    "            size = (width,height)\n",
    "            out = cv2.VideoWriter(f\"video{i}.avi\",cv2.VideoWriter_fourcc(*'DIVX'), 25, size)\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "            out.write(img)\n",
    "            while not done:\n",
    "\n",
    "                action, _ = model.predict(obs)\n",
    "                print(action)\n",
    "                obs, _, done ,_ = self.step(int(action))\n",
    "                img = obs['img']\n",
    "                img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "                out.write(img)\n",
    "            out.release()\n",
    "    \n",
    "    def get_statistic(self, model, num_games):\n",
    "        collision = 0\n",
    "        win = 0\n",
    "        destroyed = 0\n",
    "        loss = 0\n",
    "        for i in range(num_games):\n",
    "            obs = self.reset()\n",
    "            done = False\n",
    "            while not done:\n",
    "                action, _ = model.predict(obs)\n",
    "                obs, reward, done ,_ = self.step(int(action))\n",
    "                \n",
    "            if reward == -30:#win\n",
    "                collision+=1\n",
    "            elif reward == 100:# loss\n",
    "                win +=1\n",
    "            elif reward == -100:# loss\n",
    "                destroyed +=1\n",
    "            else:    #not_achieved\n",
    "                loss+=1\n",
    "        \n",
    "        print(\"Win: \",win/num_games)\n",
    "        print(\"destroyed: \", destroyed/num_games)\n",
    "        print(\"loss: \",loss/num_games)\n",
    "        print(\"collision: \",collision/num_games)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0280f0-9b53-4bd2-877e-a540c4eac603",
   "metadata": {},
   "source": [
    "## Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12befa1f-6986-44df-9d1c-112cae166704",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = CustomEnv(obstacle_turn = True,\n",
    "            vizualaze     = False, \n",
    "            Total_war     = True,\n",
    "            head_velocity = 0.005,#0.005\n",
    "            num_obs       = 1, \n",
    "            num_enemy     = 1, \n",
    "            size_obs      = [50, 60],\n",
    "            rew_col       = -70,\n",
    "            rew_win       = 100,\n",
    "            inp_dim       = 500,\n",
    "            rew_defeat    = -100,\n",
    "            steps_limit   = 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4fcd2515-0bea-481f-be58-063a03618211",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_games = 250\n",
    "filename = str(n_games)+'games_scale'+'.png'\n",
    "figure_file = 'plots/' + filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "934f73cb-0bd3-4f27-914b-8f1a43ef21a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3fa9f9fd-3853-44c3-abb7-9ae8d7938d3d",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Discrete' object has no attribute 'high'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m agent \u001b[38;5;241m=\u001b[39m \u001b[43mAgent\u001b[49m\u001b[43m(\u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.003\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.003\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreward_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcustom\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m              \u001b[49m\u001b[43minput_dims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobservation_space\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtau\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.005\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m              \u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer1_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer2_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m              \u001b[49m\u001b[43mn_actions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction_space\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/PycharmProjects/Rl/Ready/SAC/sac.py:23\u001b[0m, in \u001b[0;36mAgent.__init__\u001b[0;34m(self, alpha, beta, input_dims, tau, env, env_id, gamma, n_actions, max_size, layer1_size, layer2_size, batch_size, reward_scale)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size \u001b[38;5;241m=\u001b[39m batch_size\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_actions \u001b[38;5;241m=\u001b[39m n_actions\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor \u001b[38;5;241m=\u001b[39m ActorNetwork(alpha, input_dims, layer1_size,\n\u001b[1;32m     21\u001b[0m                           layer2_size, n_actions\u001b[38;5;241m=\u001b[39mn_actions,\n\u001b[1;32m     22\u001b[0m                           name\u001b[38;5;241m=\u001b[39menv_id \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_actor\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m---> 23\u001b[0m                           max_actions\u001b[38;5;241m=\u001b[39m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction_space\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhigh\u001b[49m)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic_1 \u001b[38;5;241m=\u001b[39m CriticNetwork(beta, input_dims, layer1_size,\n\u001b[1;32m     25\u001b[0m                               layer2_size, n_actions\u001b[38;5;241m=\u001b[39mn_actions,\n\u001b[1;32m     26\u001b[0m                               name\u001b[38;5;241m=\u001b[39menv_id \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_critic_1\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic_2 \u001b[38;5;241m=\u001b[39m CriticNetwork(beta, input_dims, layer1_size,\n\u001b[1;32m     28\u001b[0m                               layer2_size, n_actions\u001b[38;5;241m=\u001b[39mn_actions,\n\u001b[1;32m     29\u001b[0m                               name\u001b[38;5;241m=\u001b[39menv_id \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_critic_2\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Discrete' object has no attribute 'high'"
     ]
    }
   ],
   "source": [
    "agent = Agent(alpha=0.003, beta=0.003, reward_scale=2, env_id='custom',\n",
    "              input_dims=env.observation_space.shape, tau=0.005,\n",
    "              env=env, batch_size=256, layer1_size=256, layer2_size=256,\n",
    "              n_actions=env.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf35ed0-e6b3-4ff4-a8d5-3f81c4ad5da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_score = env.reward_range[0]\n",
    "score_history = []\n",
    "load_checkpoint = False\n",
    "\n",
    "if load_checkpoint:\n",
    "    agent.load_madels()\n",
    "    env.render(mode='human')\n",
    "\n",
    "steps=0\n",
    "for i in range(n_games):\n",
    "    score = 0\n",
    "    done = False\n",
    "    observation = env.reset()\n",
    "    observation = np.reshape(observation, (3, 500, 500))\n",
    "\n",
    "    while not done:\n",
    "        action = agent.choose_actions(observation)\n",
    "        observation_, reward, done, info = env.step(action)\n",
    "        observation_ = np.reshape(observation_, (3, 500, 500))\n",
    "        \n",
    "        steps += 1\n",
    "        score += reward\n",
    "        agent.remember(observation,action, reward, observation_, done)\n",
    "        if not load_checkpoint:\n",
    "            agent.learn()\n",
    "        observation = observation_\n",
    "    score_history.append(score)\n",
    "    avg_score = np.mean(score_history[-100:])\n",
    "\n",
    "    if avg_score > best_score:\n",
    "        best_score = avg_score\n",
    "        if not load_checkpoint:\n",
    "            agent.save_models()\n",
    "            \n",
    "    if not bool(i%10):clear_output()\n",
    "    print('episode', i, 'score %.1f' % avg_score,\n",
    "          'training 100 games avg %.1f' % avg_score,\n",
    "          'steps %d' % steps, env_id, 'scale', agent.scale)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fb982f-d849-43d1-8696-98dffa24a1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curve(scores, x, figure_file):\n",
    "    running_avg = np.zeros(len(scores))\n",
    "    for i in range(len(running_avg)):\n",
    "        running_avg[i] = np.mean(scores[max(0, i-100):(i+1)])\n",
    "    plt.plot(x, running_avg)\n",
    "    plt.title('Average of previos 100 scores')\n",
    "    plt.savefig(figure_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16cf0066-1a79-4b37-9052-ef16ab3f64bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [i+1 for i in range(n_games)]\n",
    "    plot_learning_curve(x, score_history, figure_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fdfee39-e03f-40be-8c20-10962818509c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2581efb-3354-440e-96ba-0aab10d78df3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a539aa9d-0384-4822-ba57-932879b59f0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ecfbb3-d74b-4da5-9adb-2a402cdb001a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e995be11-13f8-4ec1-80dd-2d7ca843f9c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b126063b-ce44-4ea4-9323-2a9a1738e70f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18236d6-2150-4a46-b658-a27cef241217",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5bc86d-dbcc-43d6-a36c-2127ffea615a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
